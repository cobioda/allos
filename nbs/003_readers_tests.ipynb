{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa98b33e0036b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc8485e6a8715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp readers_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import urllib\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aab0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def get_resource_path(filename):\n",
    "    \"\"\"Dynamically find the correct path to resources/ regardless of execution location.\"\"\"\n",
    "    # Detect execution environment\n",
    "    if \"__file__\" in globals():\n",
    "        project_root = Path(__file__).resolve().parent.parent  # Running from library (allos/)\n",
    "    else:\n",
    "        project_root = Path.cwd().parent  # Running from notebooks (nbs/)\n",
    "\n",
    "    # Construct full path\n",
    "    resource_path = project_root / \"resources\" / filename\n",
    "\n",
    "    # Debug: Print where it's looking\n",
    "    print(f\"\\nðŸ”Ž Looking for file at: {resource_path}\")\n",
    "\n",
    "    # If file is missing, print directory structure\n",
    "    if not resource_path.exists():\n",
    "        print(\"âŒ File not found!\")\n",
    "        print(f\"ðŸ“‚ Checking directory structure of: {resource_path.parent}\")\n",
    "\n",
    "        # Print all files and folders inside the expected resources directory\n",
    "        for root, dirs, files in os.walk(resource_path.parent):\n",
    "            level = root.replace(str(resource_path.parent), \"\").count(os.sep)\n",
    "            indent = \" \" * (level * 4)\n",
    "            print(f\"{indent}ðŸ“ {Path(root).name}/\")\n",
    "            for f in files:\n",
    "                print(f\"{indent}    ðŸ“„ {f}\")\n",
    "\n",
    "        raise FileNotFoundError(f\"\\nâŒ File not found at: {resource_path}\")\n",
    "\n",
    "    print(f\"âœ… File found at: {resource_path}\")\n",
    "    return resource_path\n",
    "\n",
    "# Example Usage:\n",
    "file_path = get_resource_path(\"e18.mouse.clusters.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"\\nâœ… Successfully loaded: {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7f96cb51c1da7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d693233b5eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "#| export\n",
    "def download_test_data(\n",
    "    url: str = \"https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz\",\n",
    "    output_filename: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Downloads a test data file from a specified URL, saves it one directory back in `../data/`, and extracts it.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the file to be downloaded. Defaults to a pre-defined test dataset.\n",
    "    output_filename (str, optional): Custom name for the extracted file. Defaults to \"sample_isomatrix.txt\".\n",
    "\n",
    "    Returns:\n",
    "    str: The absolute path of the extracted file if successful. If the file already exists, return its path instead.\n",
    "    \"\"\"\n",
    "    data_dir = Path(\"..\") / \"data\"\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Starting download of test data from {url}\")\n",
    "\n",
    "    # Extract filename from the URL\n",
    "    filename = url.split('/')[-1]\n",
    "    compressed_file = data_dir / filename\n",
    "\n",
    "    # Determine output file path\n",
    "    if output_filename:\n",
    "        extracted_file = data_dir / output_filename\n",
    "    else:\n",
    "        extracted_file = data_dir / \"sample_isomatrix.txt\"\n",
    "\n",
    "    # If the extracted file already exists, return its path\n",
    "    if extracted_file.exists():\n",
    "        print(f\"File already exists: {extracted_file}\")\n",
    "        return str(extracted_file.resolve())\n",
    "\n",
    "    # Download the file\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, compressed_file)\n",
    "        print(\"File downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download the file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the file\n",
    "    try:\n",
    "        with gzip.open(compressed_file, 'rb') as f_in:\n",
    "            with open(extracted_file, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"File extracted successfully: {extracted_file}\")\n",
    "        return str(extracted_file.resolve())\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract the file: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up the compressed file\n",
    "        if compressed_file.exists():\n",
    "            compressed_file.unlink()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "\n",
    "def iso_concat(data_inputs, batch_info=None, batch_type='path'):\n",
    "    \"\"\"\n",
    "    Concatenates a list of AnnData objects or paths to AnnData objects based on the union of transcriptIds,\n",
    "    while preserving geneId information which might be non-unique per transcriptId.\n",
    "    Missing values are filled with zeros. Adds a batch column to `.obs` based on the file path, obs_names, or numeric.\n",
    "\n",
    "    Parameters:\n",
    "    data_inputs (list of str or AnnData):\n",
    "        List of paths to AnnData objects or AnnData objects to concatenate.\n",
    "    batch_info (list of str, optional):\n",
    "        List of batch identifiers for each AnnData object in data_inputs.\n",
    "        If not provided, batch identifiers are extracted from file paths, obs_names, or a numeric sequence.\n",
    "    batch_type (str, optional):\n",
    "        Specifies which type of batch information to use. One of ['path', 'obs_names', 'numeric'].\n",
    "        Defaults to 'path'.\n",
    "\n",
    "    Returns:\n",
    "    AnnData:\n",
    "        A single concatenated AnnData object with harmonized features, geneId annotations, and batch info.\n",
    "    \"\"\"\n",
    "    adata_list = []\n",
    "    df_list = []\n",
    "    gene_ids = {}\n",
    "    batch_info_list = []\n",
    "\n",
    "    for i, data_input in enumerate(data_inputs):\n",
    "        # Check if the input is a path (string) or an AnnData object\n",
    "        if isinstance(data_input, str):\n",
    "            adata = sc.read_h5ad(data_input)\n",
    "            # Determine batch label based on batch_type\n",
    "            if batch_type == 'path':\n",
    "                batch = os.path.basename(data_input).split('_isomatrix')[0] if batch_info is None else batch_info[i]\n",
    "            elif batch_type == 'obs_names':\n",
    "                batch = adata.obs_names[0] if batch_info is None else batch_info[i]\n",
    "            elif batch_type == 'numeric':\n",
    "                batch = str(i)\n",
    "            else:\n",
    "                raise ValueError(\"batch_type should be 'path', 'obs_names' or 'numeric'.\")\n",
    "        elif isinstance(data_input, AnnData):\n",
    "            adata = data_input\n",
    "            # Determine batch label when input is already an AnnData object\n",
    "            if batch_type == 'obs_names':\n",
    "                batch = adata.obs_names[0] if batch_info is None else batch_info[i]\n",
    "            elif batch_type == 'numeric':\n",
    "                batch = str(i)\n",
    "            else:\n",
    "                raise ValueError(\"batch_type should be 'obs_names' or 'numeric' when passing AnnData objects.\")\n",
    "        else:\n",
    "            raise ValueError(\"data_inputs must be a list of paths to AnnData objects or AnnData objects.\")\n",
    "\n",
    "        adata_list.append(adata)\n",
    "        # Convert adata.X to a DataFrame for outer-join concatenation\n",
    "        df = pd.DataFrame(adata.X.T, index=adata.var['transcriptId'], columns=adata.obs_names)\n",
    "        df_list.append(df)\n",
    "        batch_info_list.extend([batch] * adata.n_obs)\n",
    "\n",
    "        # Map transcriptId to its geneId\n",
    "        for transcript_id, gene_id in zip(adata.var['transcriptId'], adata.var['geneId']):\n",
    "            gene_ids[transcript_id] = gene_id\n",
    "\n",
    "    # Perform an outer join on all DataFrames\n",
    "    concat_df = pd.concat(df_list, axis=1, join='outer').fillna(0)\n",
    "\n",
    "    # Create a new var DataFrame with geneIds mapped from transcriptIds\n",
    "    var_df = pd.DataFrame(index=concat_df.index)\n",
    "    var_df['geneId'] = pd.Series(gene_ids).reindex(concat_df.index)\n",
    "\n",
    "    # Build an AnnData object, transposing so obs are rows\n",
    "    concatenated_adata = sc.AnnData(X=concat_df.T, var=var_df)\n",
    "\n",
    "    # Add batch info to obs\n",
    "    concatenated_adata.obs['batch'] = batch_info_list\n",
    "\n",
    "    return concatenated_adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5425abdee359fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.sparse import csr_matrix\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "\n",
    "def read_sicelore_isomatrix(\n",
    "    file_path: str,\n",
    "    gene_id_label: str = \"geneId\",\n",
    "    transcript_id_label: str = \"transcriptId\",\n",
    "    remove_undef: bool = True,\n",
    "    sparse: bool = False\n",
    ") -> AnnData:\n",
    "    \"\"\"\n",
    "    Read a SiCeLoRe isomatrix file (tab-delimited) and convert it into a scanpy-compatible AnnData object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the isomatrix file (tab-delimited).\n",
    "    gene_id_label : str, optional\n",
    "        Row/column label used for gene IDs (default \"geneId\").\n",
    "    transcript_id_label : str, optional\n",
    "        Row/column label used for transcript IDs (default \"transcriptId\").\n",
    "    remove_undef : bool, optional\n",
    "        Whether to remove rows with transcriptId=\"undef\" (default True).\n",
    "    sparse : bool, optional\n",
    "        Whether to store the matrix in sparse format (default False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anndata.AnnData\n",
    "        An AnnData object containing numeric data in `.X` and metadata in `.var`.\n",
    "    \"\"\"\n",
    "    # Read in the file, expecting rows to be features initially\n",
    "    df = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "\n",
    "    # Optionally remove rows marked as \"undef\" in the transcript column\n",
    "    if remove_undef and (transcript_id_label in df.columns):\n",
    "        df = df[df[transcript_id_label] != \"undef\"]\n",
    "\n",
    "    # Reset and transpose so columns become features (var) and rows become observations (obs)\n",
    "    df = df.reset_index()\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Identify potential metadata rows (e.g., geneId, transcriptId, nbExons)\n",
    "    known_metadata_labels = [gene_id_label, transcript_id_label]\n",
    "    metadata_rows = [idx for idx in df.index if idx in known_metadata_labels or \"Exons\" in idx]\n",
    "\n",
    "    # Extract metadata if present\n",
    "    metadata_df = df.loc[metadata_rows] if metadata_rows else pd.DataFrame()\n",
    "    # Drop them from the numeric data\n",
    "    df = df.drop(metadata_rows, errors='ignore')\n",
    "\n",
    "    # Convert to float and optionally to a sparse matrix\n",
    "    try:\n",
    "        numeric_data = df.values.astype('float32')\n",
    "    except ValueError:\n",
    "        print(\"Error: Non-numeric data present in the DataFrame. Cannot convert to float.\")\n",
    "        return None\n",
    "\n",
    "    if sparse:\n",
    "        numeric_data = csr_matrix(numeric_data)\n",
    "\n",
    "    # Create AnnData object\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        adata = sc.AnnData(\n",
    "            X=numeric_data,\n",
    "            obs=pd.DataFrame(index=df.index.astype(str)),\n",
    "            var=pd.DataFrame(index=df.columns.astype(str))\n",
    "        )\n",
    "\n",
    "    # Attach metadata to the AnnData var\n",
    "    if not metadata_df.empty:\n",
    "        for row_label in metadata_df.index.unique():\n",
    "            adata.var[row_label] = metadata_df.loc[row_label].astype(str).values\n",
    "\n",
    "    # Make observation names unique\n",
    "    adata.obs_names_make_unique()\n",
    "\n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249c6250c040eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of test data from https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz\n",
      "File already exists: ../data/mouse_1.txt\n",
      "Test data downloaded successfully\n",
      "Starting download of test data from https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748089/suppl/GSM3748089%5F951c.isoforms.matrix.txt.gz\n",
      "File already exists: ../data/mouse_2.txt\n",
      "Test data downloaded successfully\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m mouse_data_str_2 \u001b[38;5;241m=\u001b[39m download_test_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748089/suppl/GSM3748089\u001b[39m\u001b[38;5;132;01m%5F\u001b[39;00m\u001b[38;5;124m951c.isoforms.matrix.txt.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmouse_2.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest data downloaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m mouse_1 \u001b[38;5;241m=\u001b[39m \u001b[43mread_sicelore_isomatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmouse_data_str_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m mouse_2 \u001b[38;5;241m=\u001b[39m read_sicelore_isomatrix(file_path\u001b[38;5;241m=\u001b[39mmouse_data_str_2)\n\u001b[1;32m     16\u001b[0m combined_mouse_data \u001b[38;5;241m=\u001b[39m iso_concat([mouse_1, mouse_2], batch_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mread_sicelore_isomatrix\u001b[0;34m(file_path, gene_id_label, transcript_id_label, remove_undef, sparse)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mRead a SiCeLoRe isomatrix file (tab-delimited) and convert it into a scanpy-compatible AnnData object.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    An AnnData object containing numeric data in `.X` and metadata in `.var`.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Read in the file, expecting rows to be features initially\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Optionally remove rows marked as \"undef\" in the transcript column\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_undef \u001b[38;5;129;01mand\u001b[39;00m (transcript_id_label \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "File \u001b[0;32m/data/analysis/data_mcandrew/Allos_new/allos_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/analysis/data_mcandrew/Allos_new/allos_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/data/analysis/data_mcandrew/Allos_new/allos_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/analysis/data_mcandrew/Allos_new/allos_env/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/data/analysis/data_mcandrew/Allos_new/allos_env/lib/python3.9/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/data/analysis/data_mcandrew/Allos_new/allos_env/lib/python3.9/site-packages/pandas/io/common.py:472\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    470\u001b[0m ):\n\u001b[1;32m    471\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file path or buffer object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(filepath_or_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[1;32m    475\u001b[0m     filepath_or_buffer\u001b[38;5;241m=\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    476\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    480\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = get_resource_path(\"e18.mouse.clusters.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "df['barcode'] = df.index.str.split('_').str[1]\n",
    "\n",
    "\n",
    "mouse_data_str_1 = download_test_data(output_filename='mouse_1.txt')\n",
    "print(\"Test data downloaded successfully\")\n",
    "\n",
    "\n",
    "mouse_data_str_2 = download_test_data(\"https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748089/suppl/GSM3748089%5F951c.isoforms.matrix.txt.gz\", output_filename='mouse_2.txt')\n",
    "print(\"Test data downloaded successfully\")\n",
    "\n",
    "\n",
    "mouse_1 = read_sicelore_isomatrix(file_path=mouse_data_str_1)\n",
    "mouse_2 = read_sicelore_isomatrix(file_path=mouse_data_str_2)\n",
    "\n",
    "combined_mouse_data = iso_concat([mouse_1, mouse_2], batch_type='numeric')\n",
    "\n",
    "\n",
    "combined_mouse_data.obs_names_make_unique()\n",
    "# Step 1: Remove any duplicate barcodes in the DataFrame\n",
    "df_unique = df.drop_duplicates(subset='barcode')\n",
    "\n",
    "# Step 2: Filter the DataFrame to include only the barcodes present in the AnnData object\n",
    "df_filtered = df_unique[df_unique['barcode'].isin(combined_mouse_data.obs_names)]\n",
    "\n",
    "# Step 3: Set the index of the filtered DataFrame to 'barcode' to make the merge easier\n",
    "df_filtered.set_index('barcode', inplace=True)\n",
    "\n",
    "# Step 4: Create a DataFrame from the obs DataFrame of the AnnData object to ensure the same index\n",
    "obs_df = combined_mouse_data.obs.copy()\n",
    "\n",
    "# Step 5: Initialize a new column 'cell_type' with NaN values in the obs DataFrame\n",
    "obs_df['cell_type'] = pd.NA\n",
    "\n",
    "# Step 6: Update the 'cell_type' column with values from the filtered DataFrame where indices match\n",
    "obs_df.update(df_filtered['illumina.ident'].rename('cell_type'))\n",
    "\n",
    "# Step 7: Ensure the index is unique and assign the updated DataFrame back to the obs attribute of the AnnData object\n",
    "if obs_df.index.is_unique:\n",
    "    combined_mouse_data.obs = obs_df\n",
    "else:\n",
    "    raise ValueError(\"The index of the obs DataFrame is not unique.\")\n",
    "\n",
    "# Now, the 'cell_type' column should be added to the obs DataFrame of your AnnData object\n",
    "combined_mouse_data = combined_mouse_data[~combined_mouse_data.obs['cell_type'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57231583",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_data_str_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2383ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs Ã— n_vars = 1109 Ã— 31986\n",
       "    obs: 'batch', 'cell_type'\n",
       "    var: 'geneId'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mouse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7823c3a51abf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0210500750482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dfd332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
