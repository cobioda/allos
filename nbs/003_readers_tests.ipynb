{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bfa98b33e0036b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bc8485e6a8715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp readers_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import urllib\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d142f222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/analysis/data_mcandrew/Allos_new/allos_env/lib/python3.9/site-packages/allos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import allos\n",
    "allos.__path__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73aab0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_resource_path(filename):\n",
    "    \"\"\"Dynamically find the correct path to the 'resources/' directory based on execution location.\"\"\"\n",
    "    # Detect if running from the library (allos/) or a notebook (nbs/)\n",
    "    if \"__file__\" in globals():\n",
    "        # Running from the library, path is relative to the project root\n",
    "        project_root = Path(__file__).resolve().parent\n",
    "    else:\n",
    "        # Running from a notebook, path is two levels up\n",
    "        project_root = Path.cwd().resolve().parents[0]\n",
    "\n",
    "    # Construct the full path to the resource file\n",
    "    resource_path = project_root / \"resources\" / filename\n",
    "\n",
    "    # Debug message for locating the file\n",
    "    print(f\"\\n🔎 Looking for file at: {resource_path}\")\n",
    "\n",
    "    # If the file does not exist, generate an error with the directory structure\n",
    "    if not resource_path.exists():\n",
    "        dir_structure = f\"\\n📂 Directory structure of {resource_path.parent}:\\n\"\n",
    "        \n",
    "        # Traverse and collect folder structure\n",
    "        for root, dirs, files in os.walk(resource_path.parent):\n",
    "            level = root.replace(str(resource_path.parent), \"\").count(os.sep)\n",
    "            indent = \" \" * (level * 4)\n",
    "            dir_structure += f\"{indent}📁 {Path(root).name}/\\n\"\n",
    "            for f in files:\n",
    "                dir_structure += f\"{indent}    📄 {f}\\n\"\n",
    "\n",
    "        # Raise an error including the directory structure\n",
    "        raise FileNotFoundError(f\"\\n❌ File not found at: {resource_path}\\n{dir_structure}\")\n",
    "\n",
    "    print(f\"✅ File found at: {resource_path}\")\n",
    "    return resource_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949e6d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Looking for file at: /data/analysis/data_mcandrew/allos_2/allos/resources/e18.mouse.clusters.csv\n",
      "✅ File found at: /data/analysis/data_mcandrew/allos_2/allos/resources/e18.mouse.clusters.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = get_resource_path(\"e18.mouse.clusters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7f96cb51c1da7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64d693233b5eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "#| export\n",
    "def download_test_data(\n",
    "    url: str = \"https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz\",\n",
    "    output_filename: str = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Downloads a test data file from a specified URL, saves it one directory back in `../data/`, and extracts it.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The URL of the file to be downloaded. Defaults to a pre-defined test dataset.\n",
    "    output_filename (str, optional): Custom name for the extracted file. Defaults to \"sample_isomatrix.txt\".\n",
    "\n",
    "    Returns:\n",
    "    str: The absolute path of the extracted file if successful. If the file already exists, return its path instead.\n",
    "    \"\"\"\n",
    "    data_dir = Path(\"..\") / \"data\"\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Starting download of test data from {url}\")\n",
    "\n",
    "    # Extract filename from the URL\n",
    "    filename = url.split('/')[-1]\n",
    "    compressed_file = data_dir / filename\n",
    "\n",
    "    # Determine output file path\n",
    "    if output_filename:\n",
    "        extracted_file = data_dir / output_filename\n",
    "    else:\n",
    "        extracted_file = data_dir / \"sample_isomatrix.txt\"\n",
    "\n",
    "    # If the extracted file already exists, return its path\n",
    "    if extracted_file.exists():\n",
    "        print(f\"File already exists: {extracted_file}\")\n",
    "        return str(extracted_file.resolve())\n",
    "\n",
    "    # Download the file\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, compressed_file)\n",
    "        print(\"File downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download the file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the file\n",
    "    try:\n",
    "        with gzip.open(compressed_file, 'rb') as f_in:\n",
    "            with open(extracted_file, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"File extracted successfully: {extracted_file}\")\n",
    "        return str(extracted_file.resolve())\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract the file: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up the compressed file\n",
    "        if compressed_file.exists():\n",
    "            compressed_file.unlink()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ca3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "\n",
    "def iso_concat(data_inputs, batch_info=None, batch_type='path'):\n",
    "    \"\"\"\n",
    "    Concatenates a list of AnnData objects or paths to AnnData objects based on the union of transcriptIds,\n",
    "    while preserving geneId information which might be non-unique per transcriptId.\n",
    "    Missing values are filled with zeros. Adds a batch column to `.obs` based on the file path, obs_names, or numeric.\n",
    "\n",
    "    Parameters:\n",
    "    data_inputs (list of str or AnnData):\n",
    "        List of paths to AnnData objects or AnnData objects to concatenate.\n",
    "    batch_info (list of str, optional):\n",
    "        List of batch identifiers for each AnnData object in data_inputs.\n",
    "        If not provided, batch identifiers are extracted from file paths, obs_names, or a numeric sequence.\n",
    "    batch_type (str, optional):\n",
    "        Specifies which type of batch information to use. One of ['path', 'obs_names', 'numeric'].\n",
    "        Defaults to 'path'.\n",
    "\n",
    "    Returns:\n",
    "    AnnData:\n",
    "        A single concatenated AnnData object with harmonized features, geneId annotations, and batch info.\n",
    "    \"\"\"\n",
    "    adata_list = []\n",
    "    df_list = []\n",
    "    gene_ids = {}\n",
    "    batch_info_list = []\n",
    "\n",
    "    for i, data_input in enumerate(data_inputs):\n",
    "        # Check if the input is a path (string) or an AnnData object\n",
    "        if isinstance(data_input, str):\n",
    "            adata = sc.read_h5ad(data_input)\n",
    "            # Determine batch label based on batch_type\n",
    "            if batch_type == 'path':\n",
    "                batch = os.path.basename(data_input).split('_isomatrix')[0] if batch_info is None else batch_info[i]\n",
    "            elif batch_type == 'obs_names':\n",
    "                batch = adata.obs_names[0] if batch_info is None else batch_info[i]\n",
    "            elif batch_type == 'numeric':\n",
    "                batch = str(i)\n",
    "            else:\n",
    "                raise ValueError(\"batch_type should be 'path', 'obs_names' or 'numeric'.\")\n",
    "        elif isinstance(data_input, AnnData):\n",
    "            adata = data_input\n",
    "            # Determine batch label when input is already an AnnData object\n",
    "            if batch_type == 'obs_names':\n",
    "                batch = adata.obs_names[0] if batch_info is None else batch_info[i]\n",
    "            elif batch_type == 'numeric':\n",
    "                batch = str(i)\n",
    "            else:\n",
    "                raise ValueError(\"batch_type should be 'obs_names' or 'numeric' when passing AnnData objects.\")\n",
    "        else:\n",
    "            raise ValueError(\"data_inputs must be a list of paths to AnnData objects or AnnData objects.\")\n",
    "\n",
    "        adata_list.append(adata)\n",
    "        # Convert adata.X to a DataFrame for outer-join concatenation\n",
    "        df = pd.DataFrame(adata.X.T, index=adata.var['transcriptId'], columns=adata.obs_names)\n",
    "        df_list.append(df)\n",
    "        batch_info_list.extend([batch] * adata.n_obs)\n",
    "\n",
    "        # Map transcriptId to its geneId\n",
    "        for transcript_id, gene_id in zip(adata.var['transcriptId'], adata.var['geneId']):\n",
    "            gene_ids[transcript_id] = gene_id\n",
    "\n",
    "    # Perform an outer join on all DataFrames\n",
    "    concat_df = pd.concat(df_list, axis=1, join='outer').fillna(0)\n",
    "\n",
    "    # Create a new var DataFrame with geneIds mapped from transcriptIds\n",
    "    var_df = pd.DataFrame(index=concat_df.index)\n",
    "    var_df['geneId'] = pd.Series(gene_ids).reindex(concat_df.index)\n",
    "\n",
    "    # Build an AnnData object, transposing so obs are rows\n",
    "    concatenated_adata = sc.AnnData(X=concat_df.T, var=var_df)\n",
    "\n",
    "    # Add batch info to obs\n",
    "    concatenated_adata.obs['batch'] = batch_info_list\n",
    "\n",
    "    return concatenated_adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5425abdee359fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.sparse import csr_matrix\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "\n",
    "def read_sicelore_isomatrix(\n",
    "    file_path: str,\n",
    "    gene_id_label: str = \"geneId\",\n",
    "    transcript_id_label: str = \"transcriptId\",\n",
    "    remove_undef: bool = True,\n",
    "    sparse: bool = False\n",
    ") -> AnnData:\n",
    "    \"\"\"\n",
    "    Read a SiCeLoRe isomatrix file (tab-delimited) and convert it into a scanpy-compatible AnnData object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the isomatrix file (tab-delimited).\n",
    "    gene_id_label : str, optional\n",
    "        Row/column label used for gene IDs (default \"geneId\").\n",
    "    transcript_id_label : str, optional\n",
    "        Row/column label used for transcript IDs (default \"transcriptId\").\n",
    "    remove_undef : bool, optional\n",
    "        Whether to remove rows with transcriptId=\"undef\" (default True).\n",
    "    sparse : bool, optional\n",
    "        Whether to store the matrix in sparse format (default False).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anndata.AnnData\n",
    "        An AnnData object containing numeric data in `.X` and metadata in `.var`.\n",
    "    \"\"\"\n",
    "    # Read in the file, expecting rows to be features initially\n",
    "    df = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "\n",
    "    # Optionally remove rows marked as \"undef\" in the transcript column\n",
    "    if remove_undef and (transcript_id_label in df.columns):\n",
    "        df = df[df[transcript_id_label] != \"undef\"]\n",
    "\n",
    "    # Reset and transpose so columns become features (var) and rows become observations (obs)\n",
    "    df = df.reset_index()\n",
    "    df = df.transpose()\n",
    "\n",
    "    # Identify potential metadata rows (e.g., geneId, transcriptId, nbExons)\n",
    "    known_metadata_labels = [gene_id_label, transcript_id_label]\n",
    "    metadata_rows = [idx for idx in df.index if idx in known_metadata_labels or \"Exons\" in idx]\n",
    "\n",
    "    # Extract metadata if present\n",
    "    metadata_df = df.loc[metadata_rows] if metadata_rows else pd.DataFrame()\n",
    "    # Drop them from the numeric data\n",
    "    df = df.drop(metadata_rows, errors='ignore')\n",
    "\n",
    "    # Convert to float and optionally to a sparse matrix\n",
    "    try:\n",
    "        numeric_data = df.values.astype('float32')\n",
    "    except ValueError:\n",
    "        print(\"Error: Non-numeric data present in the DataFrame. Cannot convert to float.\")\n",
    "        return None\n",
    "\n",
    "    if sparse:\n",
    "        numeric_data = csr_matrix(numeric_data)\n",
    "\n",
    "    # Create AnnData object\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        adata = sc.AnnData(\n",
    "            X=numeric_data,\n",
    "            obs=pd.DataFrame(index=df.index.astype(str)),\n",
    "            var=pd.DataFrame(index=df.columns.astype(str))\n",
    "        )\n",
    "\n",
    "    # Attach metadata to the AnnData var\n",
    "    if not metadata_df.empty:\n",
    "        for row_label in metadata_df.index.unique():\n",
    "            adata.var[row_label] = metadata_df.loc[row_label].astype(str).values\n",
    "\n",
    "    # Make observation names unique\n",
    "    adata.obs_names_make_unique()\n",
    "\n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9249c6250c040eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Looking for file at: /data/analysis/data_mcandrew/allos_2/allos/resources/e18.mouse.clusters.csv\n",
      "✅ File found at: /data/analysis/data_mcandrew/allos_2/allos/resources/e18.mouse.clusters.csv\n",
      "Starting download of test data from https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz\n",
      "File already exists: ../data/mouse_1.txt\n",
      "Test data downloaded successfully\n",
      "Starting download of test data from https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748089/suppl/GSM3748089%5F951c.isoforms.matrix.txt.gz\n",
      "File already exists: ../data/mouse_2.txt\n",
      "Test data downloaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/analysis/data_mcandrew/Allos_new/allos_env/lib/python3.9/site-packages/anndata/_core/anndata.py:1754: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = get_resource_path(\"e18.mouse.clusters.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "df['barcode'] = df.index.str.split('_').str[1]\n",
    "\n",
    "\n",
    "mouse_data_str_1 = download_test_data(output_filename='mouse_1.txt')\n",
    "print(\"Test data downloaded successfully\")\n",
    "\n",
    "\n",
    "mouse_data_str_2 = download_test_data(\"https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748089/suppl/GSM3748089%5F951c.isoforms.matrix.txt.gz\", output_filename='mouse_2.txt')\n",
    "print(\"Test data downloaded successfully\")\n",
    "\n",
    "\n",
    "mouse_1 = read_sicelore_isomatrix(file_path=mouse_data_str_1)\n",
    "mouse_2 = read_sicelore_isomatrix(file_path=mouse_data_str_2)\n",
    "\n",
    "combined_mouse_data = iso_concat([mouse_1, mouse_2], batch_type='numeric')\n",
    "\n",
    "\n",
    "combined_mouse_data.obs_names_make_unique()\n",
    "# Step 1: Remove any duplicate barcodes in the DataFrame\n",
    "df_unique = df.drop_duplicates(subset='barcode')\n",
    "\n",
    "# Step 2: Filter the DataFrame to include only the barcodes present in the AnnData object\n",
    "df_filtered = df_unique[df_unique['barcode'].isin(combined_mouse_data.obs_names)]\n",
    "\n",
    "# Step 3: Set the index of the filtered DataFrame to 'barcode' to make the merge easier\n",
    "df_filtered.set_index('barcode', inplace=True)\n",
    "\n",
    "# Step 4: Create a DataFrame from the obs DataFrame of the AnnData object to ensure the same index\n",
    "obs_df = combined_mouse_data.obs.copy()\n",
    "\n",
    "# Step 5: Initialize a new column 'cell_type' with NaN values in the obs DataFrame\n",
    "obs_df['cell_type'] = pd.NA\n",
    "\n",
    "# Step 6: Update the 'cell_type' column with values from the filtered DataFrame where indices match\n",
    "obs_df.update(df_filtered['illumina.ident'].rename('cell_type'))\n",
    "\n",
    "# Step 7: Ensure the index is unique and assign the updated DataFrame back to the obs attribute of the AnnData object\n",
    "if obs_df.index.is_unique:\n",
    "    combined_mouse_data.obs = obs_df\n",
    "else:\n",
    "    raise ValueError(\"The index of the obs DataFrame is not unique.\")\n",
    "\n",
    "# Now, the 'cell_type' column should be added to the obs DataFrame of your AnnData object\n",
    "combined_mouse_data = combined_mouse_data[~combined_mouse_data.obs['cell_type'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57231583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/analysis/data_mcandrew/allos_2/allos/data/mouse_1.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouse_data_str_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2383ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs × n_vars = 1109 × 31986\n",
       "    obs: 'batch', 'cell_type'\n",
       "    var: 'geneId'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_mouse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f7823c3a51abf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a0210500750482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dfd332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
