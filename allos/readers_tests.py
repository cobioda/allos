# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/003_readers_tests.ipynb.

# %% auto 0
__all__ = ['get_resource_path', 'download_test_data', 'iso_concat', 'read_sicelore_isomatrix']

# %% ../nbs/003_readers_tests.ipynb 2
import urllib
import urllib.request
import gzip
import shutil
from pathlib import Path



# %% ../nbs/003_readers_tests.ipynb 3
from pathlib import Path
import os
import pandas as pd


def get_resource_path(filename):
    """Dynamically find the correct path to resources/ regardless of execution location."""
    # Detect execution environment
    if "__file__" in globals():
        project_root = Path(__file__).resolve().parent.parent  # Running from library (allos/)
    else:
        project_root = Path.cwd().parent  # Running from notebooks (nbs/)

    # Construct full path
    resource_path = project_root / "resources" / filename

    # Debug: Print where it's looking
    print(f"\n🔎 Looking for file at: {resource_path}")

    # If file is missing, include directory structure in error message
    if not resource_path.exists():
        dir_structure = f"\n📂 Directory structure of {resource_path.parent}:\n"
        
        # Collect the folder and file structure
        for root, dirs, files in os.walk(resource_path.parent):
            level = root.replace(str(resource_path.parent), "").count(os.sep)
            indent = " " * (level * 4)
            dir_structure += f"{indent}📁 {Path(root).name}/\n"
            for f in files:
                dir_structure += f"{indent}    📄 {f}\n"

        # Raise an error including the directory structure
        raise FileNotFoundError(f"\n❌ File not found at: {resource_path}\n{dir_structure}")

    print(f"✅ File found at: {resource_path}")
    return resource_path







# %% ../nbs/003_readers_tests.ipynb 5
from pathlib import Path
import urllib.request
import gzip
import shutil



def download_test_data(
    url: str = "https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz",
    output_filename: str = None
) -> str:
    """
    Downloads a test data file from a specified URL, saves it dynamically based on execution environment,
    and extracts it.

    - If running from a **notebook (`nbs/`)**, saves to `../data/`.
    - If running from the **library (`allos/`)**, saves to `data/` at the project root.

    Parameters:
    url (str): The URL of the file to be downloaded.
    output_filename (str, optional): Custom name for the extracted file.

    Returns:
    str: The absolute path of the extracted file if successful. If the file already exists, return its path.
    """
    # Detect execution context (Notebook vs. Library)
    if "__file__" in globals():
        project_root = Path(__file__).resolve().parent.parent  # Running from library (allos/)
    else:
        project_root = Path.cwd().parent  # Running from notebooks (nbs/)

    # Define the correct data directory
    data_dir = project_root / "data"
    data_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists

    # Extract filename from the URL
    filename = url.split('/')[-1]
    compressed_file = data_dir / filename

    # Determine output file path
    extracted_file = data_dir / (output_filename if output_filename else "sample_isomatrix.txt")

    # Return existing file path if it already exists
    if extracted_file.exists():
        return str(extracted_file.resolve())

    # Download the file
    try:
        urllib.request.urlretrieve(url, compressed_file)
    except Exception:
        return None

    # Extract the file
    try:
        with gzip.open(compressed_file, 'rb') as f_in, open(extracted_file, 'wb') as f_out:
            shutil.copyfileobj(f_in, f_out)
        return str(extracted_file.resolve())
    except Exception:
        return None
    finally:
        if compressed_file.exists():
            compressed_file.unlink()




# %% ../nbs/003_readers_tests.ipynb 6
import os
import pandas as pd
import scanpy as sc
from anndata import AnnData

def iso_concat(data_inputs, batch_info=None, batch_type='path'):
    """
    Concatenates a list of AnnData objects or paths to AnnData objects based on the union of transcriptIds,
    while preserving geneId information which might be non-unique per transcriptId.
    Missing values are filled with zeros. Adds a batch column to `.obs` based on the file path, obs_names, or numeric.

    Parameters:
    data_inputs (list of str or AnnData):
        List of paths to AnnData objects or AnnData objects to concatenate.
    batch_info (list of str, optional):
        List of batch identifiers for each AnnData object in data_inputs.
        If not provided, batch identifiers are extracted from file paths, obs_names, or a numeric sequence.
    batch_type (str, optional):
        Specifies which type of batch information to use. One of ['path', 'obs_names', 'numeric'].
        Defaults to 'path'.

    Returns:
    AnnData:
        A single concatenated AnnData object with harmonized features, geneId annotations, and batch info.
    """
    adata_list = []
    df_list = []
    gene_ids = {}
    batch_info_list = []

    for i, data_input in enumerate(data_inputs):
        # Check if the input is a path (string) or an AnnData object
        if isinstance(data_input, str):
            adata = sc.read_h5ad(data_input)
            # Determine batch label based on batch_type
            if batch_type == 'path':
                batch = os.path.basename(data_input).split('_isomatrix')[0] if batch_info is None else batch_info[i]
            elif batch_type == 'obs_names':
                batch = adata.obs_names[0] if batch_info is None else batch_info[i]
            elif batch_type == 'numeric':
                batch = str(i)
            else:
                raise ValueError("batch_type should be 'path', 'obs_names' or 'numeric'.")
        elif isinstance(data_input, AnnData):
            adata = data_input
            # Determine batch label when input is already an AnnData object
            if batch_type == 'obs_names':
                batch = adata.obs_names[0] if batch_info is None else batch_info[i]
            elif batch_type == 'numeric':
                batch = str(i)
            else:
                raise ValueError("batch_type should be 'obs_names' or 'numeric' when passing AnnData objects.")
        else:
            raise ValueError("data_inputs must be a list of paths to AnnData objects or AnnData objects.")

        adata_list.append(adata)
        # Convert adata.X to a DataFrame for outer-join concatenation
        df = pd.DataFrame(adata.X.T, index=adata.var['transcriptId'], columns=adata.obs_names)
        df_list.append(df)
        batch_info_list.extend([batch] * adata.n_obs)

        # Map transcriptId to its geneId
        for transcript_id, gene_id in zip(adata.var['transcriptId'], adata.var['geneId']):
            gene_ids[transcript_id] = gene_id

    # Perform an outer join on all DataFrames
    concat_df = pd.concat(df_list, axis=1, join='outer').fillna(0)

    # Create a new var DataFrame with geneIds mapped from transcriptIds
    var_df = pd.DataFrame(index=concat_df.index)
    var_df['geneId'] = pd.Series(gene_ids).reindex(concat_df.index)

    # Build an AnnData object, transposing so obs are rows
    concatenated_adata = sc.AnnData(X=concat_df.T, var=var_df)

    # Add batch info to obs
    concatenated_adata.obs['batch'] = batch_info_list

    return concatenated_adata



# %% ../nbs/003_readers_tests.ipynb 7
import pandas as pd
import warnings
from scipy.sparse import csr_matrix
import scanpy as sc
from anndata import AnnData

def read_sicelore_isomatrix(
    file_path: str,
    gene_id_label: str = "geneId",
    transcript_id_label: str = "transcriptId",
    remove_undef: bool = True,
    sparse: bool = False
) -> AnnData:
    """
    Read a SiCeLoRe isomatrix file (tab-delimited) and convert it into a scanpy-compatible AnnData object.

    Parameters
    ----------
    file_path : str
        Path to the isomatrix file (tab-delimited).
    gene_id_label : str, optional
        Row/column label used for gene IDs (default "geneId").
    transcript_id_label : str, optional
        Row/column label used for transcript IDs (default "transcriptId").
    remove_undef : bool, optional
        Whether to remove rows with transcriptId="undef" (default True).
    sparse : bool, optional
        Whether to store the matrix in sparse format (default False).

    Returns
    -------
    anndata.AnnData
        An AnnData object containing numeric data in `.X` and metadata in `.var`.
    """
    # Read in the file, expecting rows to be features initially
    df = pd.read_csv(file_path, sep='\t', index_col=0)

    # Optionally remove rows marked as "undef" in the transcript column
    if remove_undef and (transcript_id_label in df.columns):
        df = df[df[transcript_id_label] != "undef"]

    # Reset and transpose so columns become features (var) and rows become observations (obs)
    df = df.reset_index()
    df = df.transpose()

    # Identify potential metadata rows (e.g., geneId, transcriptId, nbExons)
    known_metadata_labels = [gene_id_label, transcript_id_label]
    metadata_rows = [idx for idx in df.index if idx in known_metadata_labels or "Exons" in idx]

    # Extract metadata if present
    metadata_df = df.loc[metadata_rows] if metadata_rows else pd.DataFrame()
    # Drop them from the numeric data
    df = df.drop(metadata_rows, errors='ignore')

    # Convert to float and optionally to a sparse matrix
    try:
        numeric_data = df.values.astype('float32')
    except ValueError:
        print("Error: Non-numeric data present in the DataFrame. Cannot convert to float.")
        return None

    if sparse:
        numeric_data = csr_matrix(numeric_data)

    # Create AnnData object
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        adata = sc.AnnData(
            X=numeric_data,
            obs=pd.DataFrame(index=df.index.astype(str)),
            var=pd.DataFrame(index=df.columns.astype(str))
        )

    # Attach metadata to the AnnData var
    if not metadata_df.empty:
        for row_label in metadata_df.index.unique():
            adata.var[row_label] = metadata_df.loc[row_label].astype(str).values

    # Make observation names unique
    adata.obs_names_make_unique()

    return adata

