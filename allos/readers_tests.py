# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/003_readers_tests.ipynb.

# %% auto 0
__all__ = ['get_resource_path', 'download_test_data', 'iso_concat', 'read_sicelore_isomatrix']

# %% ../nbs/003_readers_tests.ipynb 2
import urllib
import urllib.request
import gzip
import shutil
from pathlib import Path



# %% ../nbs/003_readers_tests.ipynb 4
from pathlib import Path
import os
import pandas as pd


import os
from pathlib import Path

def get_resource_path(filename):
    """Dynamically find the correct path to the 'resources/' directory based on execution location."""
    # Detect if running from the library (allos/) or a notebook (nbs/)
    if "__file__" in globals():
        # Running from the library, path is relative to the project root
        project_root = Path(__file__).resolve().parent
    else:
        # Running from a notebook, path is two levels up
        project_root = Path.cwd().resolve().parents[0]

    # Construct the full path to the resource file
    resource_path = project_root / "resources" / filename

    # Debug message for locating the file
    print(f"\nðŸ”Ž Looking for file at: {resource_path}")

    # If the file does not exist, generate an error with the directory structure
    if not resource_path.exists():
        dir_structure = f"\nðŸ“‚ Directory structure of {resource_path.parent}:\n"
        
        # Traverse and collect folder structure
        for root, dirs, files in os.walk(resource_path.parent):
            level = root.replace(str(resource_path.parent), "").count(os.sep)
            indent = " " * (level * 4)
            dir_structure += f"{indent}ðŸ“ {Path(root).name}/\n"
            for f in files:
                dir_structure += f"{indent}    ðŸ“„ {f}\n"

        # Raise an error including the directory structure
        raise FileNotFoundError(f"\nâŒ File not found at: {resource_path}\n{dir_structure}")

    print(f"âœ… File found at: {resource_path}")
    return resource_path



# %% ../nbs/003_readers_tests.ipynb 7
from pathlib import Path
import urllib.request
import gzip
import shutil

def download_test_data(
    url: str = "https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM3748nnn/GSM3748087/suppl/GSM3748087%5F190c.isoforms.matrix.txt.gz",
    output_filename: str = None
) -> str:
    """
    Download test data to the correct directory, dynamically adjusting based on the execution context.

    Parameters
    ----------
    url : str
        URL to download the data from.
    output_filename : str, optional
        Name of the file to save the data as (default: name from the URL).

    Returns
    -------
    str
        Path to the downloaded file.
    """
    import os
    from pathlib import Path
    import urllib.request

    # Determine base directory based on context (notebook or CI/library)
    if "__file__" in globals():  # CI or running from library
        base_dir = Path(__file__).resolve().parent / "resources"
    else:  # Notebook context
        base_dir = Path.cwd().resolve()

    # Ensure data directory exists
    data_dir = base_dir / "data"
    data_dir.mkdir(parents=True, exist_ok=True)

    # Determine output path
    if output_filename:
        output_path = data_dir / output_filename
    else:
        output_path = data_dir / Path(url).name

    # Download file if it doesn't exist
    if not output_path.is_file():
        print(f"\nðŸ”„ Downloading {url} to {output_path}...")
        urllib.request.urlretrieve(url, output_path)
        print("âœ… Download complete.")
    else:
        print(f"âœ… File already exists at: {output_path}")

    return str(output_path.resolve())



# %% ../nbs/003_readers_tests.ipynb 8
import os
import pandas as pd
import scanpy as sc
from anndata import AnnData

def iso_concat(data_inputs, batch_info=None, batch_type='path'):
    """
    Concatenates a list of AnnData objects or paths to AnnData objects based on the union of transcriptIds,
    while preserving geneId information which might be non-unique per transcriptId.
    Missing values are filled with zeros. Adds a batch column to `.obs` based on the file path, obs_names, or numeric.

    Parameters:
    data_inputs (list of str or AnnData):
        List of paths to AnnData objects or AnnData objects to concatenate.
    batch_info (list of str, optional):
        List of batch identifiers for each AnnData object in data_inputs.
        If not provided, batch identifiers are extracted from file paths, obs_names, or a numeric sequence.
    batch_type (str, optional):
        Specifies which type of batch information to use. One of ['path', 'obs_names', 'numeric'].
        Defaults to 'path'.

    Returns:
    AnnData:
        A single concatenated AnnData object with harmonized features, geneId annotations, and batch info.
    """
    adata_list = []
    df_list = []
    gene_ids = {}
    batch_info_list = []

    for i, data_input in enumerate(data_inputs):
        # Check if the input is a path (string) or an AnnData object
        if isinstance(data_input, str):
            adata = sc.read_h5ad(data_input)
            # Determine batch label based on batch_type
            if batch_type == 'path':
                batch = os.path.basename(data_input).split('_isomatrix')[0] if batch_info is None else batch_info[i]
            elif batch_type == 'obs_names':
                batch = adata.obs_names[0] if batch_info is None else batch_info[i]
            elif batch_type == 'numeric':
                batch = str(i)
            else:
                raise ValueError("batch_type should be 'path', 'obs_names' or 'numeric'.")
        elif isinstance(data_input, AnnData):
            adata = data_input
            # Determine batch label when input is already an AnnData object
            if batch_type == 'obs_names':
                batch = adata.obs_names[0] if batch_info is None else batch_info[i]
            elif batch_type == 'numeric':
                batch = str(i)
            else:
                raise ValueError("batch_type should be 'obs_names' or 'numeric' when passing AnnData objects.")
        else:
            raise ValueError("data_inputs must be a list of paths to AnnData objects or AnnData objects.")

        adata_list.append(adata)
        # Convert adata.X to a DataFrame for outer-join concatenation
        df = pd.DataFrame(adata.X.T, index=adata.var['transcriptId'], columns=adata.obs_names)
        df_list.append(df)
        batch_info_list.extend([batch] * adata.n_obs)

        # Map transcriptId to its geneId
        for transcript_id, gene_id in zip(adata.var['transcriptId'], adata.var['geneId']):
            gene_ids[transcript_id] = gene_id

    # Perform an outer join on all DataFrames
    concat_df = pd.concat(df_list, axis=1, join='outer').fillna(0)

    # Create a new var DataFrame with geneIds mapped from transcriptIds
    var_df = pd.DataFrame(index=concat_df.index)
    var_df['geneId'] = pd.Series(gene_ids).reindex(concat_df.index)

    # Build an AnnData object, transposing so obs are rows
    concatenated_adata = sc.AnnData(X=concat_df.T, var=var_df)

    # Add batch info to obs
    concatenated_adata.obs['batch'] = batch_info_list

    return concatenated_adata



# %% ../nbs/003_readers_tests.ipynb 9
import pandas as pd
import warnings
from scipy.sparse import csr_matrix
import scanpy as sc
from anndata import AnnData
import os
import pandas as pd
import warnings
from scipy.sparse import csr_matrix
import scanpy as sc
from anndata import AnnData

def read_sicelore_isomatrix(
    file_path: str,
    gene_id_label: str = "geneId",
    transcript_id_label: str = "transcriptId",
    remove_undef: bool = True,
    sparse: bool = False
) -> AnnData:
    """
    Read a SiCeLoRe isomatrix file (tab-delimited) and convert it into a scanpy-compatible AnnData object.

    Parameters
    ----------
    file_path : str
        Path to the isomatrix file (tab-delimited).
    gene_id_label : str, optional
        Row/column label used for gene IDs (default "geneId").
    transcript_id_label : str, optional
        Row/column label used for transcript IDs (default "transcriptId").
    remove_undef : bool, optional
        Whether to remove rows with transcriptId="undef" (default True).
    sparse : bool, optional
        Whether to store the matrix in sparse format (default False).

    Returns
    -------
    anndata.AnnData
        An AnnData object containing numeric data in `.X` and metadata in `.var`.
    """
    # Validate file path
    if not file_path or not os.path.isfile(file_path):
        raise ValueError(f"Invalid or non-existent file path: {file_path}")

    try:
        # Read in the file, expecting rows to be features initially
        df = pd.read_csv(file_path, sep='\t', index_col=0)
    except Exception as e:
        print(f"Error reading file at {file_path}: {e}")
        raise

    # Optionally remove rows marked as "undef" in the transcript column
    if remove_undef and (transcript_id_label in df.columns):
        df = df[df[transcript_id_label] != "undef"]

    # Reset and transpose so columns become features (var) and rows become observations (obs)
    df = df.reset_index()
    df = df.transpose()

    # Identify potential metadata rows (e.g., geneId, transcriptId, nbExons)
    known_metadata_labels = [gene_id_label, transcript_id_label]
    metadata_rows = [idx for idx in df.index if idx in known_metadata_labels or "Exons" in idx]

    # Extract metadata if present
    metadata_df = df.loc[metadata_rows] if metadata_rows else pd.DataFrame()
    # Drop them from the numeric data
    df = df.drop(metadata_rows, errors='ignore')

    # Convert to float and optionally to a sparse matrix
    try:
        numeric_data = df.values.astype('float32')
    except ValueError:
        print(f"Error: Non-numeric data present in the DataFrame from file: {file_path}")
        raise

    if sparse:
        numeric_data = csr_matrix(numeric_data)

    # Create AnnData object
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        adata = sc.AnnData(
            X=numeric_data,
            obs=pd.DataFrame(index=df.index.astype(str)),
            var=pd.DataFrame(index=df.columns.astype(str))
        )

    # Attach metadata to the AnnData var
    if not metadata_df.empty:
        for row_label in metadata_df.index.unique():
            adata.var[row_label] = metadata_df.loc[row_label].astype(str).values

    # Make observation names unique
    adata.obs_names_make_unique()

    return adata

